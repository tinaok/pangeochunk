{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate automatically the chunk (C_1, C_2,..,C_n), which fits to total chunk element size for 'auto' (E ), If we suppose that the chunk size is well distributed in the shape(S_1,S_2,...,S_n), it should follow ;\n",
    "\n",
    "C_1:S_1=C_2:S_2= ... = C_n:S_2\n",
    "\n",
    "#coef = C_1/S_1 = C_2 / S_2 ....\n",
    "\n",
    "\n",
    "C_1 x C_2  x...  x C_n = E\n",
    "\n",
    "E is calculated by array.chunk-size/dtype.itemsize / (multiple of all the non auto chunk)\n",
    "\n",
    "Thus it can be re-written as;\n",
    "\n",
    "C_2 x S_1 = C_1  x S_2 \n",
    "    thus C_2 = C_1 x S_2 / S_1\n",
    "\n",
    "...\n",
    "\n",
    "C_n X S_1 = C_1 x S_n \n",
    "    thus C_n = C_1 x S_n / S_1\n",
    "\n",
    "...\n",
    "C_1** (n)  = E x  S_1** (n) /  (S_1 x S_2x ... S_n) \n",
    "\n",
    "\n",
    "C_1 = (E/ (S_1 x S_2x ... S_n) )** (1/2)   * S_1\n",
    "\n",
    "Once C_1 is calculated, E should be re-evaluated, and solve C_2 ....\n",
    "\n",
    "BW may be it is already done in dask somewhere?\n",
    "Also may be this can be solved using numpy (lapack)'s matrix than this way. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COPY AND PAST FROM DASK dask/dask/array/core.py  Lines 2474 to 2620 in \n",
    "\n",
    "from distributed.utils import parse_bytes\n",
    "import math\n",
    "from numbers import Number, Integral\n",
    "from dask.utils import  factors\n",
    "import numpy  as np\n",
    "\n",
    "def auto_chunks(chunks, shape, limit, dtype, previous_chunks=None):\n",
    "    \"\"\" Determine automatic chunks\n",
    "    This takes in a chunks value that contains ``\"auto\"`` values in certain\n",
    "    dimensions and replaces those values with concrete dimension sizes that try\n",
    "    to get chunks to be of a certain size in bytes, provided by the ``limit=``\n",
    "    keyword.  If multiple dimensions are marked as ``\"auto\"`` then they will\n",
    "    all respond to meet the desired byte limit, trying to respect the aspect\n",
    "    ratio of their dimensions in ``previous_chunks=``, if given.\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunks: Tuple\n",
    "        A tuple of either dimensions or tuples of explicit chunk dimensions\n",
    "        Some entries should be \"auto\"\n",
    "    shape: Tuple[int]\n",
    "    limit: int, str\n",
    "        The maximum allowable size of a chunk in bytes\n",
    "    previous_chunks: Tuple[Tuple[int]]\n",
    "    See also\n",
    "    --------\n",
    "    normalize_chunks: for full docstring and parameters\n",
    "    \"\"\"\n",
    "    if previous_chunks is not None:\n",
    "        previous_chunks = tuple(\n",
    "            c if isinstance(c, tuple) else (c,) for c in previous_chunks\n",
    "        )\n",
    "    chunks = list(chunks)\n",
    "\n",
    "    autos = {i for i, c in enumerate(chunks) if c == \"auto\"}\n",
    "    if not autos:\n",
    "        return tuple(chunks)\n",
    "\n",
    "    if limit is None:\n",
    "        limit = config.get(\"array.chunk-size\")\n",
    "    if isinstance(limit, str):\n",
    "        limit = parse_bytes(limit)\n",
    "\n",
    "    if dtype is None:\n",
    "        raise TypeError(\"DType must be known for auto-chunking\")\n",
    "\n",
    "    if dtype.hasobject:\n",
    "        raise NotImplementedError(\n",
    "            \"Can not use auto rechunking with object dtype. \"\n",
    "            \"We are unable to estimate the size in bytes of object data\"\n",
    "        )\n",
    "\n",
    "    for x in tuple(chunks) + tuple(shape):\n",
    "        if (\n",
    "            isinstance(x, Number)\n",
    "            and np.isnan(x)\n",
    "            or isinstance(x, tuple)\n",
    "            and np.isnan(x).any()\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Can not perform automatic rechunking with unknown \"\n",
    "                \"(nan) chunk sizes.%s\" % unknown_chunk_message\n",
    "            )\n",
    "\n",
    "    limit = max(1, limit)\n",
    "\n",
    "    largest_block = np.prod(\n",
    "        [cs if isinstance(cs, Number) else max(cs) for cs in chunks if cs != \"auto\"]\n",
    "    )\n",
    "\n",
    "    if previous_chunks:\n",
    "        # Base ideal ratio on the median chunk size of the previous chunks\n",
    "        result = {a: np.median(previous_chunks[a]) for a in autos}\n",
    "\n",
    "        ideal_shape = []\n",
    "        for i, s in enumerate(shape):\n",
    "            chunk_frequencies = frequencies(previous_chunks[i])\n",
    "            mode, count = max(chunk_frequencies.items(), key=lambda kv: kv[1])\n",
    "            if mode > 1 and count >= len(previous_chunks[i]) / 2:\n",
    "                ideal_shape.append(mode)\n",
    "            else:\n",
    "                ideal_shape.append(s)\n",
    "\n",
    "        # How much larger or smaller the ideal chunk size is relative to what we have now\n",
    "        multiplier = (\n",
    "            limit / dtype.itemsize / largest_block / np.prod(list(result.values()))\n",
    "        )\n",
    "        last_multiplier = 0\n",
    "        last_autos = set()\n",
    "\n",
    "        while (\n",
    "            multiplier != last_multiplier or autos != last_autos\n",
    "        ):  # while things change\n",
    "            last_multiplier = multiplier  # record previous values\n",
    "            last_autos = set(autos)  # record previous values\n",
    "\n",
    "            # Expand or contract each of the dimensions appropriately\n",
    "            for a in sorted(autos):\n",
    "                proposed = result[a] * multiplier ** (1 / len(autos))\n",
    "                if proposed > shape[a]:  # we've hit the shape boundary\n",
    "                    autos.remove(a)\n",
    "                    largest_block *= shape[a]\n",
    "                    chunks[a] = shape[a]\n",
    "                    del result[a]\n",
    "                else:\n",
    "                    result[a] = round_to(proposed, ideal_shape[a])\n",
    "\n",
    "            # recompute how much multiplier we have left, repeat\n",
    "            multiplier = (\n",
    "                limit / dtype.itemsize / largest_block / np.prod(list(result.values()))\n",
    "            )\n",
    "\n",
    "        for k, v in result.items():\n",
    "            chunks[k] = v\n",
    "        return tuple(chunks)\n",
    "\n",
    "    else:\n",
    "        size = (limit / dtype.itemsize / largest_block) ** (1 / len(autos))\n",
    "        small = [i for i in autos if shape[i] < size]\n",
    "        if small:\n",
    "            for i in small:\n",
    "                chunks[i] = (shape[i],)\n",
    "            return auto_chunks(chunks, shape, limit, dtype)\n",
    "\n",
    "        for i in autos:\n",
    "            chunks[i] = round_to(size, shape[i])\n",
    "\n",
    "        return tuple(chunks)\n",
    "\n",
    "\n",
    "def round_to(c, s):\n",
    "    \"\"\" Return a chunk dimension that is close to an even multiple or factor\n",
    "    We want values for c that are nicely aligned with s.\n",
    "    If c is smaller than s then we want the largest factor of s that is less than the\n",
    "    desired chunk size, but not less than half, which is too much.  If no such\n",
    "    factor exists then we just go with the original chunk size and accept an\n",
    "    uneven chunk at the end.\n",
    "    If c is larger than s then we want the largest multiple of s that is still\n",
    "    smaller than c.\n",
    "    \"\"\"\n",
    "    if c <= s:\n",
    "        try:\n",
    "            return max(f for f in factors(s) if c / 2 <= f <= c)\n",
    "        except ValueError:  # no matching factors within factor of two\n",
    "            return max(1, int(c))\n",
    "    else:\n",
    "        return c // s * s\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COPY AND PAST FROM DASK dask/dask/array/core.py  Lines 2474 to 2620 \n",
    "### and slight change after the last 'else'\n",
    "\n",
    "def auto_newchunks(chunks, shape, limit, dtype, previous_chunks=None):\n",
    "    \"\"\" Determine automatic chunks\n",
    "    This takes in a chunks value that contains ``\"auto\"`` values in certain\n",
    "    dimensions and replaces those values with concrete dimension sizes that try\n",
    "    to get chunks to be of a certain size in bytes, provided by the ``limit=``\n",
    "    keyword.  If multiple dimensions are marked as ``\"auto\"`` then they will\n",
    "    all respond to meet the desired byte limit, trying to respect the aspect\n",
    "    ratio of their dimensions in ``previous_chunks=``, if given.\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunks: Tuple\n",
    "        A tuple of either dimensions or tuples of explicit chunk dimensions\n",
    "        Some entries should be \"auto\"\n",
    "    shape: Tuple[int]\n",
    "    limit: int, str\n",
    "        The maximum allowable size of a chunk in bytes\n",
    "    previous_chunks: Tuple[Tuple[int]]\n",
    "    See also\n",
    "    --------\n",
    "    normalize_chunks: for full docstring and parameters\n",
    "    \"\"\"\n",
    "    if previous_chunks is not None:\n",
    "        previous_chunks = tuple(\n",
    "            c if isinstance(c, tuple) else (c,) for c in previous_chunks\n",
    "        )\n",
    "    chunks = list(chunks)\n",
    "\n",
    "    autos = {i for i, c in enumerate(chunks) if c == \"auto\"}\n",
    "    if not autos:\n",
    "        return tuple(chunks)\n",
    "\n",
    "    if limit is None:\n",
    "        limit = config.get(\"array.chunk-size\")\n",
    "    if isinstance(limit, str):\n",
    "        limit = parse_bytes(limit)\n",
    "\n",
    "    if dtype is None:\n",
    "        raise TypeError(\"DType must be known for auto-chunking\")\n",
    "\n",
    "    if dtype.hasobject:\n",
    "        raise NotImplementedError(\n",
    "            \"Can not use auto rechunking with object dtype. \"\n",
    "            \"We are unable to estimate the size in bytes of object data\"\n",
    "        )\n",
    "\n",
    "    for x in tuple(chunks) + tuple(shape):\n",
    "        if (\n",
    "            isinstance(x, Number)\n",
    "            and np.isnan(x)\n",
    "            or isinstance(x, tuple)\n",
    "            and np.isnan(x).any()\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Can not perform automatic rechunking with unknown \"\n",
    "                \"(nan) chunk sizes.%s\" % unknown_chunk_message\n",
    "            )\n",
    "\n",
    "    limit = max(1, limit)\n",
    "\n",
    "    largest_block = np.prod(\n",
    "        [cs if isinstance(cs, Number) else max(cs) for cs in chunks if cs != \"auto\"]\n",
    "    )\n",
    "\n",
    "    if previous_chunks:\n",
    "        # Base ideal ratio on the median chunk size of the previous chunks\n",
    "        result = {a: np.median(previous_chunks[a]) for a in autos}\n",
    "\n",
    "        ideal_shape = []\n",
    "        for i, s in enumerate(shape):\n",
    "            chunk_frequencies = frequencies(previous_chunks[i])\n",
    "            mode, count = max(chunk_frequencies.items(), key=lambda kv: kv[1])\n",
    "            if mode > 1 and count >= len(previous_chunks[i]) / 2:\n",
    "                ideal_shape.append(mode)\n",
    "            else:\n",
    "                ideal_shape.append(s)\n",
    "\n",
    "        # How much larger or smaller the ideal chunk size is relative to what we have now\n",
    "        multiplier = (\n",
    "            limit / dtype.itemsize / largest_block / np.prod(list(result.values()))\n",
    "        )\n",
    "        last_multiplier = 0\n",
    "        last_autos = set()\n",
    "\n",
    "        while (\n",
    "            multiplier != last_multiplier or autos != last_autos\n",
    "        ):  # while things change\n",
    "            last_multiplier = multiplier  # record previous values\n",
    "            last_autos = set(autos)  # record previous values\n",
    "\n",
    "            # Expand or contract each of the dimensions appropriately\n",
    "            for a in sorted(autos):\n",
    "                proposed = result[a] * multiplier ** (1 / len(autos))\n",
    "                if proposed > shape[a]:  # we've hit the shape boundary\n",
    "                    autos.remove(a)\n",
    "                    largest_block *= shape[a]\n",
    "                    chunks[a] = shape[a]\n",
    "                    del result[a]\n",
    "                else:\n",
    "                    result[a] = round_to(proposed, ideal_shape[a])\n",
    "\n",
    "            # recompute how much multiplier we have left, repeat\n",
    "            multiplier = (\n",
    "                limit / dtype.itemsize / largest_block / np.prod(list(result.values()))\n",
    "            )\n",
    "\n",
    "        for k, v in result.items():\n",
    "            chunks[k] = v\n",
    "        return tuple(chunks)\n",
    "\n",
    "    else:\n",
    "        size = (limit / dtype.itemsize / largest_block) ** (1 / len(autos))\n",
    "        small = [i for i in autos if shape[i] < size]\n",
    "        if small:\n",
    "            for i in small:\n",
    "                chunks[i] = (shape[i],)\n",
    "            return auto_chunks(chunks, shape, limit, dtype)\n",
    "\n",
    "#        for i in autos:\n",
    "#            chunks[i] = round_to(size, shape[i])\n",
    "\n",
    "\n",
    "\n",
    "        chunk_block = limit / dtype.itemsize\n",
    "        print('after small',chunks)\n",
    "\n",
    "        for i in autos:\n",
    "            print('in autosi',i,autos,chunks)\n",
    "            #auto_chunk_block = chunk_block/(np.prod of non auto chunk values)\n",
    "            autosleft = {i for i, c in enumerate(chunks) if c == \"auto\"}\n",
    "            print('autosleft',autosleft)\n",
    "            nonresolved = chunk_block\n",
    "            for i, c in enumerate(chunks):\n",
    "               if c != \"auto\":\n",
    "                   nonresolved=nonresolved /c\n",
    "            print('nonresolved_block',nonresolved)\n",
    "\n",
    "            coef3= np.prod(shape)/(nonresolved)\n",
    "            coef=int(coef3**(1/len(autosleft)))\n",
    "            size=int(shape[i]/coef)\n",
    "            print('coef,size',coef,size)\n",
    "            chunks[i] = round_to(size, shape[i])\n",
    "            print('computed chunk at i',i,chunks[i])\n",
    "\n",
    "        return tuple(chunks)\n",
    "            \n",
    "\n",
    "\n",
    "def round_to(c, s):\n",
    "    \"\"\" Return a chunk dimension that is close to an even multiple or factor\n",
    "    We want values for c that are nicely aligned with s.\n",
    "    If c is smaller than s then we want the largest factor of s that is less than the\n",
    "    desired chunk size, but not less than half, which is too much.  If no such\n",
    "    factor exists then we just go with the original chunk size and accept an\n",
    "    uneven chunk at the end.\n",
    "    If c is larger than s then we want the largest multiple of s that is still\n",
    "    smaller than c.\n",
    "    \"\"\"\n",
    "    if c <= s:\n",
    "        try:\n",
    "            return max(f for f in factors(s) if c / 2 <= f <= c)\n",
    "        except ValueError:  # no matching factors within factor of two\n",
    "            return max(1, int(c))\n",
    "    else:\n",
    "        return c // s * s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---test with shape= (20834, 384, 320) and limit= 258MB\n",
      "--chunk computed from dask (318, 192, 160) the size is : 78151680 bytes, thus ratio is: 0.30291348837209303\n",
      "--chunk computed from dask (318, 192, 160) the size is : 78151680 bytes, thus ratio is: 0.30291348837209303\n",
      "---test with shape= (20834, 384, 320) and limit= 124MB\n",
      "--chunk computed from dask (249, 192, 160) the size is : 61194240 bytes, thus ratio is: 0.49350193548387095\n",
      "--chunk computed from dask (249, 192, 160) the size is : 61194240 bytes, thus ratio is: 0.49350193548387095\n",
      "---test with shape= (20834, 384, 320) and limit= 64MB\n",
      "--chunk computed from dask (199, 192, 160) the size is : 48906240 bytes, thus ratio is: 0.76416\n",
      "--chunk computed from dask (199, 192, 160) the size is : 48906240 bytes, thus ratio is: 0.76416\n",
      "---test with shape= (20834, 384, 320) and limit= 378MB\n",
      "--chunk computed from dask (384, 384, 320) the size is : 377487360 bytes, thus ratio is: 0.9986438095238095\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-79805cc60ba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--chunk computed from dask'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'the size is :'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'bytes, thus ratio is:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mparse_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--chunk computed from dask'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunkori\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'the size is :'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunkori\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'bytes, thus ratio is:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunkori\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mparse_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m  \u001b[0;31m#   print('--chunk computed from dask',chunknew,'the size is :',np.prod(chunknew)*dtype.itemsize,'bytes, thus ratio is:',np.prod(chunknew)*dtype.itemsize/parse_bytes(limit))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pangeochunk/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2909\u001b[0m     \"\"\"\n\u001b[1;32m   2910\u001b[0m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0;32m-> 2911\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pangeochunk/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'tuple'"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "from distributed.utils import parse_bytes\n",
    "import dask\n",
    "import numpy  as np\n",
    "#from core_auto_chunks_try import auto_chunks\n",
    "#from core_auto_chunks import auto_chunks\n",
    "\n",
    "\n",
    "limit_list=['258MB','124MB','64MB','378MB','512MB']\n",
    "\n",
    "\n",
    "timesteps=20834\n",
    "lat=320\n",
    "lon=384\n",
    "shape=(timesteps,lon,lat)\n",
    "chunks=(\"auto\",\"auto\",\"auto\")\n",
    "previous_chunks=None\n",
    "\n",
    "for limit in limit_list: \n",
    "    print('---test with shape=',shape,'and limit=',limit)\n",
    "    dask.config.set({\"array.chunk-size\": limit})\n",
    "    x =da.random.RandomState(0).standard_normal(shape, chunks=chunks  )\n",
    "    dtype=x.dtype\n",
    "    chunkori=auto_chunks(chunks, shape, limit, dtype=dtype, previous_chunks=previous_chunks)\n",
    "#    chunknew=auto_newchunks(chunks, shape, limit, dtype=dtype, previous_chunks=previous_chunks)\n",
    "\n",
    "\n",
    "    print('--chunk computed from dask',x.chunksize,'the size is :',np.prod(x.chunksize)*dtype.itemsize,'bytes, thus ratio is:',np.prod(x.chunksize)*dtype.itemsize/parse_bytes(limit))\n",
    "    print('--chunk computed from dask',chunkori,'the size is :',np.prod(chunkori)*dtype.itemsize,'bytes, thus ratio is:',np.prod(chunkori)*dtype.itemsize/parse_bytes(limit))\n",
    " #   print('--chunk computed from dask',chunknew,'the size is :',np.prod(chunknew)*dtype.itemsize,'bytes, thus ratio is:',np.prod(chunknew)*dtype.itemsize/parse_bytes(limit))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeochunk",
   "language": "python",
   "name": "pangeochunk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
